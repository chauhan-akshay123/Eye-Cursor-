
Eye Controlled Mouse - Code Explanation

1. Imports
-----------------
import cv2
import mediapipe as mp
import pyautogui

- `cv2`: OpenCV library for computer vision tasks like video capture, image processing, and displaying frames.
- `mediapipe`: A framework from Google that includes machine learning models for facial and hand landmark detection.
- `pyautogui`: A Python module used for controlling the mouse and keyboard. In this case, it's used to move the mouse and simulate clicks based on facial movements.

2. Initialize Camera and Face Mesh
-----------------------------------
cam = cv2.VideoCapture(0)
face_mesh = mp.solutions.face_mesh.FaceMesh(refine_landmarks=True)
screen_w, screen_h = pyautogui.size()

- `cv2.VideoCapture(0)`: Opens the default camera (index 0) to capture video frames.
- `face_mesh`: Initializes Mediapipe's FaceMesh model for detecting facial landmarks.
- `screen_w, screen_h = pyautogui.size()`: Retrieves the dimensions of the screen to map detected eye landmarks to mouse movements.

3. Frame Capture and Processing
-------------------------------
while True:
    _, frame = cam.read()
    frame = cv2.flip(frame, 1)
    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
    output = face_mesh.process(rgb_frame)

- `cam.read()`: Captures a frame from the webcam.
- `cv2.flip`: Flips the frame horizontally to create a mirror effect.
- `cv2.cvtColor`: Converts frame from BGR format (used by OpenCV) to RGB (used by Mediapipe).
- `face_mesh.process`: Passes the frame to Mediapipe's face mesh for processing and detection of face landmarks.

4. Eye Landmark Processing and Mouse Movement
----------------------------------------------
if landmark_points:
    landmarks = landmark_points[0].landmark
    for id, landmark in enumerate(landmarks[474:478]):
        x = int(landmark.x * frame_w)
        y = int(landmark.y * frame_h)
        cv2.circle(frame, (x, y), 3, (0, 255, 0))
        if id == 1:
            screen_x = screen_w * landmark.x
            screen_y = screen_h * landmark.y
            pyautogui.moveTo(screen_x, screen_y)

- Detects and processes eye landmarks (indices 474 to 478).
- Maps the normalized x and y coordinates of these landmarks to the screen dimensions.
- `pyautogui.moveTo`: Moves the mouse cursor based on the calculated coordinates.

5. Blink Detection and Click Simulation
----------------------------------------
left = [landmarks[145], landmarks[159]]
for landmark in left:
    x = int(landmark.x * frame_w)
    y = int(landmark.y * frame_h)
    cv2.circle(frame, (x, y), 3, (0, 255, 255))

if (left[0].y - left[1].y) < 0.004:
    pyautogui.click()
    pyautogui.sleep(1)

- `left = [landmarks[145], landmarks[159]]`: These are the landmarks corresponding to the left eye.
- It calculates the distance between the top and bottom points of the left eye.
- If the distance is below a certain threshold, a blink is detected and a mouse click is triggered using `pyautogui.click()`.

Code Functionality Summary:
The camera captures video frames.
The frames are processed to detect facial landmarks using Mediapipe's FaceMesh.
The specific eye landmarks are used to move the mouse cursor. The movement is based on the relative position of the eye in the frame mapped to the screen size.
The blink is detected using two specific landmarks around the eye, and a mouse click is simulated when a blink is detected.
This code essentially uses your eyes (their position and blink) to control the mouse cursor and simulate clicks, creating an eye-controlled mouse system.

------More Detailed------

Let's break down the code you provided, explaining the purpose and functionality of each section:

Imports:
python
Copy code
import cv2
import mediapipe as mp
import pyautogui
cv2: OpenCV library for computer vision tasks like video capture, image processing, and displaying frames.
mediapipe: A framework from Google that includes machine learning models for facial and hand landmark detection. Specifically, the face_mesh module is used for detecting face landmarks.
pyautogui: A Python module used for controlling the mouse and keyboard. In this case, it's used to move the mouse and simulate mouse clicks based on facial movements.
Initialize Camera and Face Mesh:
python
Copy code
cam = cv2.VideoCapture(0)
face_mesh = mp.solutions.face_mesh.FaceMesh(refine_landmarks=True)
screen_w, screen_h = pyautogui.size()
cv2.VideoCapture(0): Opens the default camera (index 0) to capture video frames.
face_mesh = mp.solutions.face_mesh.FaceMesh(refine_landmarks=True): Initializes Mediapipe's FaceMesh model with refine_landmarks=True, enabling a more detailed set of facial landmarks.
screen_w, screen_h = pyautogui.size(): Retrieves the dimensions of the screen to help map the detected eye landmarks to mouse movements.
Start a Loop to Capture and Process Frames:
python
Copy code
while True:
    _, frame = cam.read()
    frame = cv2.flip(frame, 1)
    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
    output = face_mesh.process(rgb_frame)
cam.read(): Captures a frame from the webcam.
cv2.flip(frame, 1): Flips the frame horizontally to create a mirror effect, so movements in the frame match the direction of actual movements.
cv2.cvtColor(frame, cv2.COLOR_BGR2RGB): Converts the frame from BGR color format (used by OpenCV) to RGB (required by Mediapipe).
face_mesh.process(rgb_frame): Passes the RGB frame to Mediapipe's face_mesh for landmark detection. The result, output, contains facial landmarks.
Get the Detected Landmarks:
python
Copy code
landmark_points = output.multi_face_landmarks
frame_h, frame_w, _ = frame.shape
output.multi_face_landmarks: Holds the face landmark detection results (a list of 468 facial landmarks). If no face is detected, this value is None.
frame.shape: Gets the height and width of the current frame to map normalized landmarks (ranging from 0 to 1) to pixel coordinates.
Eye Landmark Processing and Mouse Movement:
python
Copy code
if landmark_points:
    landmarks = landmark_points[0].landmark
    for id, landmark in enumerate(landmarks[474:478]):
        x = int(landmark.x * frame_w)
        y = int(landmark.y * frame_h)
        cv2.circle(frame, (x, y), 3, (0, 255, 0))
        if id == 1:
            screen_x = screen_w * landmark.x
            screen_y = screen_h * landmark.y
            pyautogui.moveTo(screen_x, screen_y)
If landmarks are detected (if landmark_points:), the script accesses the first face's landmark points (landmarks = landmark_points[0].landmark).
The loop for id, landmark in enumerate(landmarks[474:478]) iterates over eye landmarks from index 474 to 478 (these correspond to the eyes in Mediapipe's face mesh model).
For each landmark, x and y are calculated by multiplying the normalized landmark values (ranging from 0 to 1) by the frame's width (frame_w) and height (frame_h), converting them into pixel coordinates.
A green circle (cv2.circle(frame, (x, y), 3, (0, 255, 0))) is drawn at each of the selected eye landmark positions.
When id == 1, the mouse cursor is moved to the position of the eye landmark using pyautogui.moveTo(screen_x, screen_y). The screen_x and screen_y values are computed by scaling the normalized landmark coordinates to the screen resolution.
Detecting Eye Blinks for Clicks:
python
Copy code
    left = [landmarks[145], landmarks[159]]
    for landmark in left:
        x = int(landmark.x * frame_w)
        y = int(landmark.y * frame_h)
        cv2.circle(frame, (x, y), 3, (0, 255, 255))
    if (left[0].y - left[1].y) < 0.004:
        pyautogui.click()
        pyautogui.sleep(1)
The left list stores two specific landmarks for the left eye (landmarks[145] and landmarks[159]), which are useful for detecting blinks.
These two landmarks correspond to points around the upper and lower eyelids. By monitoring their relative vertical positions, you can detect when the eye blinks (i.e., when the distance between them decreases).
A yellow circle is drawn around these landmarks.
The line if (left[0].y - left[1].y) < 0.004: checks if the difference in the vertical position (y values) of the two landmarks is small enough to detect an eye blink. If true, it simulates a mouse click (pyautogui.click()) and pauses for 1 second (pyautogui.sleep(1)) to avoid multiple clicks in quick succession.
Displaying the Frame and Exiting:
python
Copy code
cv2.imshow('Eye Controlled Mouse', frame)
cv2.waitKey(1)
cv2.imshow('Eye Controlled Mouse', frame): Displays the video feed with drawn landmarks.
cv2.waitKey(1): Waits for a key press, but in this loop, it effectively just waits 1 millisecond before moving on to the next frame.